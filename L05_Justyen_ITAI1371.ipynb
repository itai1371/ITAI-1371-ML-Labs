{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d2ba1f",
   "metadata": {},
   "source": [
    "# Module 05 Lab - Data Preparation\n",
    "\n",
    "**Objective:** To learn and apply the most common data preparation techniques. Raw data is rarely ready for a machine learning model. This process, also called preprocessing, is one of the most critical steps in the entire ML workflow.\n",
    "\n",
    "**In this lab, you will write more of the code.\n",
    "\n",
    "** Read the explanations and then complete the tasks in the code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdc40f",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Initial Look\n",
    "\n",
    "We will continue using the Titanic dataset because it has the exact problems we need to solve: missing values and non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b131be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Values Before Cleaning ---\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Let's look at the missing values\n",
    "print(\"--- Missing Values Before Cleaning ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd252f22",
   "metadata": {},
   "source": [
    "## Part 2: Handling Missing Values (Imputation)\n",
    "\n",
    "**Concept:** Most machine learning models cannot handle missing values (`NaN`). We must deal with them. Dropping the rows is an option, but you lose data. A better way is **imputation**, which means filling in the missing values with a calculated guess.\n",
    "\n",
    "Common imputation strategies:*   \n",
    "**Mean:** Fill with the average value. Good for normally distributed data.*   \n",
    "**Median:** Fill with the middle value. Better for skewed data or data with outliers (like `Fare`).*   \n",
    "**Mode:** Fill with the most frequent value. Used for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7fcbd",
   "metadata": {},
   "source": [
    "### Task 1: Impute the 'Age' Column\n",
    "\n",
    "The 'Age' column is missing many values. Since age can be skewed (e.g., by a few very old passengers), using the **median** is a robust choice.\n",
    "\n",
    "**Your Task:** Calculate the median of the 'Age' column and use the `.fillna()` method to replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b831a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median age: 28.0\n",
      "Missing values in 'Age' after imputation:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# --- ENTER YOUR CODE HERE ---\n",
    "# 1. Calculate the median of the 'Age' column\n",
    "# median_age = ...\n",
    "median_age = df['Age'].median()\n",
    "print(\"Median age:\", median_age)\n",
    "# 2. Fill the missing values in 'Age' with the median\n",
    "# df['Age'].fillna(..., inplace=True)\n",
    "#df['Age'].fillna(median_age, inplace =True)\n",
    "#Ran into lots of errors/warnings, so looked up new code\n",
    "df['Age'] = df['Age'].fillna(median_age)\n",
    "# 3. Verify that there are no more missing values in 'Age'\n",
    "print(\"Missing values in 'Age' after imputation:\")\n",
    "print(df['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ffa53",
   "metadata": {},
   "source": [
    "## Part 3: Encoding Categorical Features\n",
    "\n",
    "**Concept:** Machine learning models are mathematical, so they need numbers, not text. We need to convert categorical columns (like 'Sex' and 'Embarked') into a numerical format. The most common method is \n",
    "\n",
    "**One-Hot Encoding**.One-Hot Encoding takes a column with `N` categories and turns it into `N` new columns, each with a `1` or `0`. For example, the 'Sex' column (`male`, `female`) becomes two new columns: `Sex_male` and `Sex_female`.\n",
    "\n",
    "Pandas has a convenient function called `pd.get_dummies()` that does this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a1001",
   "metadata": {},
   "source": [
    "### Task 2: One-Hot Encode Categorical Columns\n",
    "\n",
    "**Your Task:** Use `pd.get_dummies()` to encode the 'Sex' and 'Embarked' columns. Make sure to drop the original columns after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e9cb687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name   Age  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris  22.0      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n",
      "2                             Heikkinen, Miss. Laina  26.0      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n",
      "4                           Allen, Mr. William Henry  35.0      0      0   \n",
      "\n",
      "             Ticket     Fare Cabin  Sex_male  Embarked_Q  Embarked_S  \n",
      "0         A/5 21171   7.2500   NaN      True       False        True  \n",
      "1          PC 17599  71.2833   C85     False       False       False  \n",
      "2  STON/O2. 3101282   7.9250   NaN     False       False        True  \n",
      "3            113803  53.1000  C123     False       False        True  \n",
      "4            373450   8.0500   NaN      True       False        True  \n",
      "--- List of Columns after OHC ---\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Sex_male', 'Embarked_Q', 'Embarked_S'],\n",
      "      dtype='object')\n",
      "--- Observation ---\n",
      "It seems like Sex_female and Embarked_C are gone due to redundancy. Sex_male = 1 means male. Sex_male = 0 means female.\n"
     ]
    }
   ],
   "source": [
    "# --- ENTER YOUR CODE HERE ---\n",
    "# 1. Use get_dummies to create new columns for 'Sex' and 'Embarked'\n",
    "#    Set `drop_first=True` to avoid multicollinearity (a statistical issue), which drops one of the new columns (e.g., just having `Sex_male` is enough to know if someone is female).\n",
    "# encoded_df = pd.get_dummies(df, columns=[... , ...], drop_first=True)\n",
    "encoded_df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "# 2. Display the first few rows of the new DataFrame to see the new columns\n",
    "print(encoded_df.head())\n",
    "#Checking columns afterwards\n",
    "print(\"--- List of Columns after OHC ---\")\n",
    "print(encoded_df.columns)\n",
    "print(\"--- Observation ---\")\n",
    "print(\"It seems like Sex_female and Embarked_C are gone due to redundancy. Sex_male = 1 means male. Sex_male = 0 means female.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33e837",
   "metadata": {},
   "source": [
    "## Part 4: Feature Scaling\n",
    "\n",
    "**Concept:** Many models are sensitive to the scale of the features. For example, `Age` (from 0-80) and `Fare` (from 0-512) are on very different scales. This can cause the model to incorrectly believe that `Fare` is a more important feature simply because its values are larger.\n",
    "\n",
    "**Feature Scaling** solves this by putting all features on a similar scale. A common method is \n",
    "\n",
    "**Standardization** (`StandardScaler` in scikit-learn), which rescales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Important:** You only scale your numerical features, not your target variable or your newly encoded categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53890f57",
   "metadata": {},
   "source": [
    "### Task 3: Scale the 'Age' and 'Fare' Columns\n",
    "\n",
    "**Your Task:** Use `StandardScaler` from `sklearn.preprocessing` to scale the 'Age' and 'Fare' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7b866cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name       Age  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris -0.565736      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  0.663861      1      0   \n",
      "2                             Heikkinen, Miss. Laina -0.258337      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  0.433312      1      0   \n",
      "4                           Allen, Mr. William Henry  0.433312      0      0   \n",
      "\n",
      "             Ticket      Fare Cabin  Sex_male  Embarked_Q  Embarked_S  \n",
      "0         A/5 21171 -0.502445   NaN      True       False        True  \n",
      "1          PC 17599  0.786845   C85     False       False       False  \n",
      "2  STON/O2. 3101282 -0.488854   NaN     False       False        True  \n",
      "3            113803  0.420730  C123     False       False        True  \n",
      "4            373450 -0.486337   NaN      True       False        True  \n",
      "--- Age and Fare Values After Scaling ---\n",
      "Age    -1.196200e-17\n",
      "Fare    3.987333e-18\n",
      "dtype: float64\n",
      "Age     1.000562\n",
      "Fare    1.000562\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- ENTER YOUR CODE HERE ---\n",
    "# 1. Create an instance of the StandardScaler\n",
    "# scaler = ...\n",
    "scaler = StandardScaler()\n",
    "# 2. Select the columns to scale\n",
    "# columns_to_scale = ['Age', 'Fare']\n",
    "columns_to_scale = ['Age', 'Fare']\n",
    "# 3. Fit the scaler to the data and transform it\n",
    "#    Note: We are using the `encoded_df` from the previous step if you created it.\n",
    "# encoded_df[columns_to_scale] = scaler.fit_transform(encoded_df[columns_to_scale])\n",
    "encoded_df[columns_to_scale] = scaler.fit_transform(encoded_df[columns_to_scale])\n",
    "# 4. Display the first few rows to see the scaled data\n",
    "# print(encoded_df.head())\n",
    "print(encoded_df.head())\n",
    "#Checking values\n",
    "print(\"--- Age and Fare Values After Scaling ---\")\n",
    "print(encoded_df[['Age', 'Fare']].mean())\n",
    "print(encoded_df[['Age', 'Fare']].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c20ef",
   "metadata": {},
   "source": [
    "## üìù Knowledge Check\n",
    "\n",
    "**Instructions:** Answer the following questions in this markdown cell.\n",
    "\n",
    "1.  **Why is it often better to impute missing values with the median instead of the mean?**\n",
    "- It's better because there are possible outliers, such as some people being in their late ages or some being very young.\n",
    "2.  **Explain in your own words what One-Hot Encoding does and why it is necessary.**\n",
    "- It's important because machines can't read or process text values, so we assign them numbers instead.\n",
    "3.  **Would you need to apply Feature Scaling to a Decision Tree model?** Why or why not? (Hint: Think about how a Decision Tree makes its splits).\n",
    "- No, because Decision Trees do not care about distance nor would the relative order of these numbers change once the scaling occurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
